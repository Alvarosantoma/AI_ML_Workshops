{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Personalize boto3 Client\n",
    "\n",
    "All of the code that we are using in this lab is Python, but any langugage supported by SageMaker could be used. In this initial piece of code we are loading in the library dependencies that we need for the rest of the lab:\n",
    "\n",
    "- *boto3* - standard Pything CLI library for the AWS SDK\n",
    "- *json* - used to manipulate JSON structures used by our API calls\n",
    "- *numpy* and *pandas* - standard libraries used by Data Scientists everywhere\n",
    "- *time*, *datetime* and *pytz* - used for some time manipulation calls\n",
    "\n",
    "These client handlers will be used throughout the lab, and you will see examples of other client handlers being instantiated for services such as Amazon S3 and IAM.  For ease of repeat use, we are storing some variables as we go along, as that we you can come back and skip the lengthy model creation cells and go straight to forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "forecast = boto3.client(service_name='forecast') \n",
    "forecastquery = boto3.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of the timestamp - easily done, but check on how your data actually calculates the week number - there are different methods out there, so make sure you choose the right one.  For now we'll just go with the first Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"201536\"\n",
    "r = datetime.strptime(d + '-1', \"%Y%W-%w\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see how to get a date from this then, assuming that Monday is the first day of the week (the \"-1\" part) then we can apply it to the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WeeklySalesData_FY16-FY18.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['Cal Year'].astype(str)\n",
    "                               + df['Cal Week'].astype(str).add('-1') ,format='%Y%W-%w')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drop the columns that we don't need.  Other than the Timestamp value that we just created, we need the other fields that Forecast needs for a RETAIL domain Predictor, namely:\n",
    "\n",
    "- item_id <- \"Item SKU\"\n",
    "- demand <- \"Units Sold\"\n",
    "- location <- \"Store Code\"\n",
    "\n",
    "So we drop the other columns first, then rename the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Financial Year', 'Cal Year', 'Cal Week', 'Region', 'Location', 'Item Name'], axis=1)\n",
    "df = df.rename(columns={\"Store Code\": \"location\", \"Item SKU\": \"item_id\",\n",
    "                        \"Units Sold\": \"demand\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the parsed dataset.  Split the dataset based upon timestamp into two files - training and test - with out test set being just 3 months of whole 3 years worth of data.  We could just train on a subset of the data, but we're going to use all of it.  Note, we're stripping out the header row from the output files, as the Amazon Forecast documentation recommends that we do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df[(df['timestamp'] >= '2015-03-01') & (df['timestamp'] <= '2018-05-31')]\n",
    "test_set = df[(df['timestamp'] >= '2018-06-01') & (df['timestamp'] <= '2018-08-31')]\n",
    "\n",
    "training_set.to_csv(\"item-demand-train.csv\", header=False, index=False)\n",
    "test_set.to_csv(\"item-demand-validation.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Bucket for Our Data\n",
    "\n",
    "Amazon Forecast only supports the uploading of data from an S3 bucket. Hence, you need to create a new bucket or re-use an existing one. If you need to create one the you *MUST* name your bucket before running this Code cell by editing the value for bucket in the code below. You need to ensure that the bucket name is globally unique; for this lab we recommend using your name or initials, followed by -forecast-retail-data, as that is likely to be unique\n",
    "\n",
    "If the bucket already exists - such as if you execute this code cell a second time - then it will not create a new bucket, and will not make any changes to the existing bucket. If this happens unexpectedly then please check your own S3 page in the console to ensure that the bucket is in your account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ajk-forecast-retail-data\"           # replace with the name of your S3 bucket\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "if boto3.resource('s3').Bucket(bucket_name).creation_date is None:\n",
    "    s3.create_bucket(ACL = \"private\", Bucket = bucket_name, CreateBucketConfiguration={\n",
    "        'LocationConstraint': 'eu-west-1'})\n",
    "    print(\"Creating bucket: {}\".format(bucket_name))\n",
    "else:\n",
    "    print(\"Bucket {} already exists\".format(bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our bucket, we can upload our training file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"data/item-demand-train.csv\"\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(\"item-demand-train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset Group and Dataset \n",
    "\n",
    "In Amazon Forecast, a dataset is a collection of file(s) which contain data that is relevant for a forecasting task. A dataset must conform to a schema provided by Amazon Forecast.\n",
    "\n",
    "More details about Domain and dataset type can be found on the documentation. For this example, we are using RETAIL domain with 3 required attributes: timestamp, demand and item_id.  We're also going to add in a recommended optional field, location, as our dataset consists of multiple store locations and we want to be able to forecast for each one.\n",
    "\n",
    "It is important to also convey how Amazon Forecast can understand your time-series information. That the cell immediately below does that, defining a weekly dataset and a specific timestamp format, and the next one then configures your variable names for the Project, DatasetGroup, and Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FREQUENCY = \"W\" \n",
    "TIMESTAMP_FORMAT = \"yyyy-MM-dd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'retail_food_forecastdemo'\n",
    "datasetName= project+'_ds'\n",
    "datasetGroupName= project +'_dsg'\n",
    "s3DataPath = \"s3://\"+bucket_name+\"/\"+key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=datasetGroupName,\n",
    "                                                              Domain=\"RETAIL\",\n",
    "                                                             )\n",
    "datasetGroupArn = create_dataset_group_response['DatasetGroupArn']\n",
    "forecast.describe_dataset_group(DatasetGroupArn=datasetGroupArn)\n",
    "%store datasetGroupArn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Schema\n",
    "\n",
    "We now specify the schema, which tells Forecat how to parse the data.  This is the order of our training set columns, and we must ensure that these match the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "     \"Attributes\": [\n",
    "        {\n",
    "           \"AttributeName\": \"location\",\n",
    "           \"AttributeType\": \"string\"\n",
    "        },\n",
    "        {\n",
    "           \"AttributeName\": \"item_id\",\n",
    "           \"AttributeType\": \"string\"\n",
    "        },\n",
    "        {\n",
    "           \"AttributeName\": \"demand\",\n",
    "           \"AttributeType\": \"float\"\n",
    "        },\n",
    "        {\n",
    "           \"AttributeName\": \"timestamp\",\n",
    "           \"AttributeType\": \"timestamp\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName=datasetName,\n",
    "                    DataFrequency=DATASET_FREQUENCY, \n",
    "                    Schema = schema\n",
    ")\n",
    "\n",
    "datasetArn = response['DatasetArn']\n",
    "forecast.describe_dataset(DatasetArn=datasetArn)\n",
    "%store datasetArn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dataset to Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.update_dataset_group(DatasetGroupArn=datasetGroupArn, DatasetArns=[datasetArn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IAM Role for Forecast\n",
    "Like many AWS services, Forecast will need to assume an IAM role in order to interact with your S3 resources securely. The code below will create the role and it will be used later for accessing your data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "role_name = \"ForecastRoleRetailDemo\"\n",
    "\n",
    "try:\n",
    "    role_arn = iam.get_role(RoleName = role_name)[\"Role\"]['Arn']\n",
    "    print (\"Role alread exists: \", role_arn)\n",
    "except:\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "              \"Effect\": \"Allow\",\n",
    "              \"Principal\": {\n",
    "                \"Service\": \"forecast.amazonaws.com\"\n",
    "              },\n",
    "              \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    create_role_response = iam.create_role(\n",
    "        RoleName = role_name,\n",
    "        AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    "    )\n",
    "\n",
    "    # AmazonForecastFullAccess gives our role everything it needs to call itself\n",
    "    policy_arn = \"arn:aws:iam::aws:policy/AmazonForecastFullAccess\"\n",
    "    iam.attach_role_policy(\n",
    "        RoleName = role_name,\n",
    "        PolicyArn = policy_arn\n",
    "    )\n",
    "\n",
    "    # Now add S3 support, and for here we're just going for full access\n",
    "    iam.attach_role_policy(\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "        RoleName=role_name\n",
    "    )\n",
    "    time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "    role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "    print(\"New role created: \", role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Import Job\n",
    "\n",
    "Now that Forecast knows how to understand the CSV we are providing, the next step is to import the data from S3 into Amazon Forecaast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetImportJobName = 'FOOD_STORE_DSIMPORT_JOB_TARGET'\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName=datasetImportJobName,\n",
    "                                                          DatasetArn=datasetArn,\n",
    "                                                          DataSource= {\n",
    "                                                              \"S3Config\" : {\n",
    "                                                                 \"Path\":s3DataPath,\n",
    "                                                                 \"RoleArn\": role_arn\n",
    "                                                              } \n",
    "                                                          },\n",
    "                                                          TimestampFormat=TIMESTAMP_FORMAT\n",
    "                                                         )\n",
    "\n",
    "ds_import_job_arn=ds_import_job_response['DatasetImportJobArn']\n",
    "print(ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of dataset, when the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on the data size. It can take between 3 and 10 mins to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    dataImportStatus = forecast.describe_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)\n",
    "    \n",
    "    dataset_import_job = dataImportStatus[\"DatasetImportJobName\"]\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - dataImportStatus[\"CreationTime\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataImportStatus:\n",
    "        status = dataImportStatus[\"Status\"]\n",
    "        print(\"DatasetImportJob: {}   (elapsed = {})\".format(status, elapsed))\n",
    "    else:\n",
    "        # Not sure if this happens in Forecasr\n",
    "        status = dataImportStatus[\"latestDatasetImportJobRun\"][\"Status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Predictor\n",
    "\n",
    "Here we will once again define your dataset information and then start building your model or predictor.\n",
    "\n",
    "Forecast horizon is the number of number of time points to predicted in the future. For weekly data, a value of 13 means 13 weeks, which is what we want to forecast. Our example is weekly data, and we want to try and predict up to 13 weeks ahead.  We are going to use DeepAR+, as our dataset is actually fully populated with no gaps - if it was sparsely populated then we'd have considered NPTS, and if it was in the middle we'd have tried Prophet.\n",
    "\n",
    "Of course, you could have left AutoML to do its job, but that takes more time, and in this case it would have returned DeepAR+, but I suggest you play around with some HPO to further optimise the accuracy of the results, as well as extending the backtest window to more and/or longer windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictorName= project+'_deeparp_algo'\n",
    "forecastHorizon = 13\n",
    "algorithmArn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_predictor_response=forecast.create_predictor(PredictorName=predictorName, \n",
    "                                      AlgorithmArn=algorithmArn,\n",
    "                                      ForecastHorizon=forecastHorizon,\n",
    "                                      PerformAutoML=False,\n",
    "                                      PerformHPO=False,\n",
    "                                      EvaluationParameters= {\"NumberOfBacktestWindows\": 1, \n",
    "                                                             \"BackTestWindowOffset\": 13}, \n",
    "                                      InputDataConfig= {\"DatasetGroupArn\": datasetGroupArn},\n",
    "                                      FeaturizationConfig= {\"ForecastFrequency\": \"W\",\n",
    "                                                            \"ForecastDimensions\": [\"location\"]}\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_arn=create_predictor_response['PredictorArn']\n",
    "%store predictor_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the predictor. When the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on data size, model selection and hyper parameters，it can take 10 mins to more than one hour to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    predictorStatus = forecast.describe_predictor(PredictorArn=predictor_arn)\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - predictorStatus[\"CreationTime\"]\n",
    "    status = predictorStatus[\"Status\"]\n",
    "    print(\"CreatePredictor: {}   (elapsed = {})\".format(status, elapsed))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Forecast\n",
    "\n",
    "Now create a forecast using the model that was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastName = project+'_deeparp_algo_forecast'\n",
    "create_forecast_response=forecast.create_forecast(ForecastName=forecastName,\n",
    "                                                  PredictorArn=predictor_arn)\n",
    "forecast_arn = create_forecast_response['ForecastArn']\n",
    "%store forecast_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of the forecast process, when the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can continue to next steps. Depending on data size, model selection and hyper parameters，it can take 10 mins to more than one hour to be **ACTIVE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    forecastStatus = forecast.describe_forecast(ForecastArn=forecast_arn)\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - forecastStatus[\"CreationTime\"]\n",
    "    status = forecastStatus[\"Status\"]\n",
    "    print(\"CreateForecast: {}   (elapsed = {})\".format(status, elapsed))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Forecast\n",
    "\n",
    "Once created, the forecast results are ready and you query it for particular results - here we'll just go and see how well we forecasted one item across all stores, but we could always add **location** to the *Filters* to get a forecast for a specific location/item combination.  We can do this because when we created the Predictor we added in location as a specific dimension that we were interested in; if we hadn't done this at that time then you couldn't then query on location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemToForecast = \"30295920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(forecast_arn)\n",
    "print()\n",
    "forecastResponse = forecastquery.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\": itemToForecast},\n",
    "    StartDate=\"2018-06-04T00:00:00\",\n",
    "    EndDate=\"2018-08-20T00:00:00\"\n",
    ")\n",
    "print(forecastResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Actual Results\n",
    "At the beginning of this notebook we created a file of observed values, we are now going to select a given date and customer from that dataframe and are going to plot the actual usage data for that customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = pd.read_csv(\"item-demand-validation.csv\", names=['location','item','demand', 'timestamp'])\n",
    "actual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only grab values for item 30295920.  The single prediction forecast that we just did is only for a single *item_id*, so will return the totals across all stores for each weekly time slot.  Hence, to graph this properly with the actual values we need to summarise the actuals across all stores too.  If you changed the filter in the previous step to also forecast for locations then add the additional condition to the *item* and *timestamp* filter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = actual_df[(actual_df['item'] == itemToForecast) \n",
    "                      & (actual_df['timestamp'] <= '2018-08-20')]\n",
    "actual_df = actual_df.drop(['item'], axis=1)\n",
    "actual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we group by timestamp and sum up the other fields - this sums the demand and the location fields, but by then dropping location we are left with a single row per timestamp, will a rolled-up demand across all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = actual_df.groupby('timestamp').sum().drop('location', axis=1)\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Prediction\n",
    "\n",
    "Next we need to convert the JSON response from the Predictor to a dataframe that we can plot - remember that this is a summary across all locations for this item if you did not add in the *location* filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_p10 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p10'])\n",
    "prediction_df_p10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_p10.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above merely did the p10 values, now do the same for p50 and p90."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_p50 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p50'])\n",
    "prediction_df_p90 = pd.DataFrame.from_dict(forecastResponse['Forecast']['Predictions']['p90'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Prediction to Actual Results\n",
    "\n",
    "After obtaining the dataframes the next task is to plot them together to determine the best fit.  We start by creating a dataframe to house our content, and have a **source** colume to indicate which dataframe it came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['timestamp', 'value', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the observed values into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in newdf.iterrows():\n",
    "    clean_timestamp = parse(index)\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp, 'value' : row['demand'], 'source': 'actual'},\n",
    "                                   ignore_index=True)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add in the P10, P50, and P90 Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in prediction_df_p10.iterrows():\n",
    "    clean_timestamp = parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p10'} , ignore_index=True)\n",
    "for index, row in prediction_df_p50.iterrows():\n",
    "    clean_timestamp = parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p50'} , ignore_index=True)\n",
    "for index, row in prediction_df_p90.iterrows():\n",
    "    clean_timestamp = parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'source': 'p90'} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the whole table around the *source* and *timestamp* fields, which gives us a usable (and plottable) table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = results_df.pivot(columns='source', values='value', index=\"timestamp\")\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Forecast Export\n",
    "\n",
    "To do any proper analysis of the results we really need to use a BI tool of some sort, so we will prepare a full export for our forecast horizon - this should give us all items for all store locations, which we can then look at in Excel, Quicksight or any other tool that can import CSV.\n",
    "\n",
    "Check the status of the export job. When the status change from **CREATE_IN_PROGRESS** to **ACTIVE**, we can launch our BI tool of choice and start exploring. This can take 2 mins to more than 10 minutes to be **ACTIVE** for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastExportName = project + '_forecast_export'\n",
    "s3ExportPath = \"s3://\" + bucket_name + \"/forecasts\"\n",
    "create_forecast_export_response=forecast.create_forecast_export_job(ForecastExportJobName=forecastExportName,\n",
    "                                                             ForecastArn=forecast_arn,\n",
    "                                                             Destination={\n",
    "                                                                 'S3Config': {\n",
    "                                                                     'Path': s3ExportPath,\n",
    "                                                                     'RoleArn': role_arn\n",
    "                                                                 }\n",
    "                                                             }\n",
    "                                                        )\n",
    "forecastExport_arn = create_forecast_export_response['ForecastExportJobArn']\n",
    "%store forecastExport_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 60*60 # 1 hour\n",
    "while time.time() < max_time:\n",
    "    forecastExportStatus = forecast.describe_forecast_export_job(ForecastExportJobArn=forecastExport_arn)\n",
    "    now = datetime.now(pytz.utc)\n",
    "    elapsed = now - forecastExportStatus[\"CreationTime\"]\n",
    "    status = forecastExportStatus[\"Status\"]\n",
    "    print(\"ForecastExportJob: {}   (elapsed = {})\".format(status, elapsed))\n",
    "\n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Forecast with Known Demand\n",
    "\n",
    "As before with the plot for a single item across all stores, we need to merge in the actual known values - that way we can visualise how accurate these forecasts actually were.  Note that we rename/reformat columns between our two dataframes for the sake of consistency, and sort the various *df.head()* outputs so that you can visually check that the correct values are being merged across.  Clearly, they are, but this helps you see that it really is doing what you think.  We start by reading in the actual demand values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = pd.read_csv(\"item-demand-validation.csv\", names=['location','item_id','demand', 'date']).sort_values(by=['date', 'location', 'item_id'])\n",
    "actual_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load in generated forecast file - Note, export filename has timestamp-base suffix, so please go check the name in the S3 console.  Then, we change the date format to be the same as the date in the item validation frame for easy merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportS3Filename = s3ExportPath + \"/\" + forecastExportName + \"_2019-12-01T21-31-51Z_part0.csv\"  # CHANGE THIS NAME\n",
    "exportDf = pd.read_csv(exportS3Filename).sort_values(by=['date', 'location', 'item_id'])\n",
    "\n",
    "exportDf['date'] = pd.to_datetime(exportDf['date']).dt.strftime('%Y-%m-%d')\n",
    "exportDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.merge(exportDf, actual_df, on=['date', 'location', 'item_id']).sort_values(by=['date', 'location', 'item_id'])\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our single merged dataframe, we can write it back to S3 and start diving into it with our chosen tooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.to_csv(\"merged-forecast-with-actuals.csv\", header=True, index=False)\n",
    "key=\"forecasts/merged-forecast-with-actuals.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(\"merged-forecast-with-actuals.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
